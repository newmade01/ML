#확률적 경사 하강법
최적의 값을 찾아줌 , SGD(stochastic gradient Descent)
- 랜덤, 기울기, 하강하는 방 => 가장 가파른 경사를 조금씩 내려감
- 가중치, b절편을 유지하며, 업데이트
- 점진적인 학습 OR 온라인 학습
- 훈련방법, 최적화 방법
- 여러번 조금씩 경사를 따라 내려감=> 모든 데이터를 훈련(epoch=1)
  - 확률적 경사 하강법: 샘플을 하나씩 꺼내서 내려감
  - 미니 배치 경사 하강법: 여러개씩 꺼내서 내려감 (갯수: 하이퍼파라미터, 2의 배수)
  - 배치 경사 하강법: 모두 다 꺼내서 내려감 (문제점: 메모리 등 한정적인 자원)
## 손실함수 
  loss 값을 구함 (구간을 가지는 정확도는 연속이 아니기때문에 손실함수로 사용 불가 = 미분 불가능)
- **로지스틱 손실함수(=이진크로스엔트로피)**
  - 회귀모델(성능지표+손실함수) but, **분류일땐** 불가능
  - 평균 절댓값 오차, 제곱 오차 => 미분가능
  - 손실함수 = 측정지표
  - 정확도: 성능측정 => 로지스틱 손실함수로 미분하여 최적화
  - 가중치, 계수값을 변경하여 계산
    - 타겟=1, -log(예측확률)
    - 타겟=0, -log(1-예측확률)